{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.layers import Embedding\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "path = \"../data/neg\" \n",
    "files= os.listdir(path)\n",
    "\n",
    "neg = []\n",
    "for file in files:\n",
    "    position = path+'/'+ file\n",
    "\n",
    "    with open(position, \"r\") as f:\n",
    "        data = f.read()\n",
    "        neg.append(data)\n",
    "#neg = ','.join(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/pos\" \n",
    "files= os.listdir(path)\n",
    "\n",
    "pos = []\n",
    "for file in files:\n",
    "    position = path+'/'+ file\n",
    "\n",
    "    with open(position, \"r\") as f:\n",
    "        data = f.read()\n",
    "        pos.append(data)\n",
    "#pos = ','.join(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Data Exploration and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. You can use binary encoding for the sentiments , i.e y = 1 for positive sentiments and y = −1 for negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.DataFrame(pos)\n",
    "df_pos['y'] = 1\n",
    "df_neg = pd.DataFrame(neg)\n",
    "df_neg['y'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assume nothing . \\nthe phrase is perhaps one o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plot : derek zoolander is a male model . \\nhe ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i actually am a fan of the original 1961 or so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a movie that's been as highly built up as the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\" good will hunting \" is two movies in one : ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>synopsis : when a meteorite crashlands in the ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>it's now the anniversary of the slayings of ju...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>coinciding with the emerging popularity of mov...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>and now the high-flying hong kong style of fil...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>battlefield long , boring and just plain stupi...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  y\n",
       "0     assume nothing . \\nthe phrase is perhaps one o...  1\n",
       "1     plot : derek zoolander is a male model . \\nhe ...  1\n",
       "2     i actually am a fan of the original 1961 or so...  1\n",
       "3     a movie that's been as highly built up as the ...  1\n",
       "4      \" good will hunting \" is two movies in one : ...  1\n",
       "...                                                 ... ..\n",
       "1995  synopsis : when a meteorite crashlands in the ... -1\n",
       "1996  it's now the anniversary of the slayings of ju... -1\n",
       "1997  coinciding with the emerging popularity of mov... -1\n",
       "1998  and now the high-flying hong kong style of fil... -1\n",
       "1999  battlefield long , boring and just plain stupi... -1\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df_pos,df_neg],axis = 0).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. The data are pretty clean. Remove the punctuation and numbers from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = ','.join(neg)\n",
    "neg_no_num = re.sub('\\d', '', neg)\n",
    "neg_process = re.sub('\\W', ' ', neg_no_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = ','.join(pos)\n",
    "pos_no_num = re.sub('\\d', '', pos)\n",
    "pos_process = re.sub('\\W', ' ', pos_no_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iii. The name of each text file starts with cv number. Use text files 0-699 in each class for training and 700-999 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test for neg\n",
    "path = \"../data/neg\" \n",
    "files= os.listdir(path)\n",
    "\n",
    "neg_train = []\n",
    "neg_test = []\n",
    "for file in files:\n",
    "    position = path+'/'+ file\n",
    "    if int(file[2:5]) < 700:\n",
    "        with open(position, \"r\") as f:\n",
    "            data = f.read()\n",
    "            neg_train.append(data)\n",
    "    else:\n",
    "        with open(position, \"r\") as f:\n",
    "            data = f.read()\n",
    "            neg_test.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test for pos\n",
    "path = \"../data/pos\" \n",
    "files= os.listdir(path)\n",
    "\n",
    "pos_train = []\n",
    "pos_test = []\n",
    "for file in files:\n",
    "    position = path+'/'+ file\n",
    "    if int(file[2:5]) < 700:\n",
    "        with open(position, \"r\") as f:\n",
    "            data = f.read()\n",
    "            pos_train.append(data)\n",
    "    else:\n",
    "        with open(position, \"r\") as f:\n",
    "            data = f.read()\n",
    "            pos_test.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = pd.DataFrame(pos_train)\n",
    "pos_train['y'] = 1\n",
    "neg_train = pd.DataFrame(neg_train)\n",
    "neg_train['y'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : derek zoolander is a male model . \\nhe ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\" good will hunting \" is two movies in one : ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the story of us , a rob reiner film , is the s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" the fighting sullivans \" contains a major p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>george little ( jonathan lipnicki ) wants a li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>synopsis : when a meteorite crashlands in the ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>it's now the anniversary of the slayings of ju...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>coinciding with the emerging popularity of mov...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>and now the high-flying hong kong style of fil...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>battlefield long , boring and just plain stupi...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  y\n",
       "0     plot : derek zoolander is a male model . \\nhe ...  1\n",
       "1      \" good will hunting \" is two movies in one : ...  1\n",
       "2     the story of us , a rob reiner film , is the s...  1\n",
       "3      \" the fighting sullivans \" contains a major p...  1\n",
       "4     george little ( jonathan lipnicki ) wants a li...  1\n",
       "...                                                 ... ..\n",
       "1395  synopsis : when a meteorite crashlands in the ... -1\n",
       "1396  it's now the anniversary of the slayings of ju... -1\n",
       "1397  coinciding with the emerging popularity of mov... -1\n",
       "1398  and now the high-flying hong kong style of fil... -1\n",
       "1399  battlefield long , boring and just plain stupi... -1\n",
       "\n",
       "[1400 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = pd.concat([pos_train,neg_train], axis = 0).reset_index().drop('index', axis=1)\n",
    "train_set.columns = ['review', 'y']\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test = pd.DataFrame(pos_test)\n",
    "pos_test['y'] = 1\n",
    "neg_test = pd.DataFrame(neg_test)\n",
    "neg_test['y'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assume nothing . \\nthe phrase is perhaps one o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i actually am a fan of the original 1961 or so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a movie that's been as highly built up as the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anastasia contains something that has been lac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>before you read my review , you gotta know tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>martial arts master steven seagal ( not to men...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>\" tarzan and the lost city \" is one of the mo...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>vegas vacation is the fourth film starring che...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>seen december 2 , 1997 at 6 : 50 p . m . at th...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>it's difficult to expect much from a director ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  y\n",
       "0    assume nothing . \\nthe phrase is perhaps one o...  1\n",
       "1    i actually am a fan of the original 1961 or so...  1\n",
       "2    a movie that's been as highly built up as the ...  1\n",
       "3    anastasia contains something that has been lac...  1\n",
       "4    before you read my review , you gotta know tha...  1\n",
       "..                                                 ... ..\n",
       "595  martial arts master steven seagal ( not to men... -1\n",
       "596   \" tarzan and the lost city \" is one of the mo... -1\n",
       "597  vegas vacation is the fourth film starring che... -1\n",
       "598  seen december 2 , 1997 at 6 : 50 p . m . at th... -1\n",
       "599  it's difficult to expect much from a director ... -1\n",
       "\n",
       "[600 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.concat([pos_test,neg_test], axis = 0).reset_index().drop('index', axis=1)\n",
    "test_set.columns = ['review', 'y']\n",
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iv. Count the number of unique words in the whole dataset (train + test) and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_set['review'])):\n",
    "    train_set['review'][i] = re.sub('\\d', '', train_set['review'][i])\n",
    "    train_set['review'][i] = re.sub('\\W', ' ', train_set['review'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_set['review'])):\n",
    "    test_set['review'][i] = re.sub('\\d', '', test_set['review'][i])\n",
    "    test_set['review'][i] = re.sub('\\W', ' ', test_set['review'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = pd.concat([train_set,test_set],axis=0).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              479801.0\n",
       "the            76529.0\n",
       "and            35576.0\n",
       "of             34123.0\n",
       "a              38108.0\n",
       "                ...   \n",
       "gyllenhall         2.0\n",
       "hindi              1.0\n",
       "vetter             1.0\n",
       "immuno             1.0\n",
       "professing         1.0\n",
       "Length: 39212, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = whole_data.review.apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0)\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### v. Calculate the average review length and the standard deviation of review lengths. Report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "for i in range(len(whole_data)):\n",
    "    length_num = len(whole_data['review'][i].split())\n",
    "    length.append(length_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "665.567"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average review length\n",
    "np.mean(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293.6649579214381"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard deviation review length\n",
    "np.std(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vi. Plot the histogram of review lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 80., 536., 762., 401., 132.,  51.,  22.,  13.,   0.,   3.]),\n",
       " array([  17. ,  261.2,  505.4,  749.6,  993.8, 1238. , 1482.2, 1726.4,\n",
       "        1970.6, 2214.8, 2459. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vii. To represent each text (= data point), there are many ways. In NLP/Deep Learning terminology, this task is called tokenization. It is common to represent text using popularity/ rank of words in text. The most common word in the text will be represented as 1, the second most common word will be represented as 2, etc. Tokenize each text document using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = pd.DataFrame(word_freq)\n",
    "word_freq['represent number'] = 0\n",
    "for i in range(len(word_freq)):\n",
    "    word_freq['represent number'][i] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>represent number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>479801.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>76529.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>35576.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>34123.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>38108.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gyllenhall</th>\n",
       "      <td>2.0</td>\n",
       "      <td>39207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hindi</th>\n",
       "      <td>1.0</td>\n",
       "      <td>39208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vetter</th>\n",
       "      <td>1.0</td>\n",
       "      <td>39209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immuno</th>\n",
       "      <td>1.0</td>\n",
       "      <td>39210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>professing</th>\n",
       "      <td>1.0</td>\n",
       "      <td>39211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39212 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0  represent number\n",
       "            479801.0                 0\n",
       "the          76529.0                 1\n",
       "and          35576.0                 2\n",
       "of           34123.0                 3\n",
       "a            38108.0                 4\n",
       "...              ...               ...\n",
       "gyllenhall       2.0             39207\n",
       "hindi            1.0             39208\n",
       "vetter           1.0             39209\n",
       "immuno           1.0             39210\n",
       "professing       1.0             39211\n",
       "\n",
       "[39212 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### viii. Select a review length L that 70% of the reviews have a length below it. If you feel more adventurous, set the threshold to 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "759.3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(length, 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ix. Truncate reviews longer than L words and zero-pad reviews shorter than L so that all texts (= data points) are of length L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data['length'] = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Truncate_whole_set = whole_data[whole_data['length'] < 759.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = word_freq.reset_index()\n",
    "word_freq.columns = ['words', 'freq', 'represent number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_whole = pd.DataFrame(Truncate_whole_set['review'].str.split(' ')).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_whole_set = []\n",
    "for i in range(len(Truncate_whole_set)):\n",
    "    df2 = pd.DataFrame(df_whole['review'][i])\n",
    "    df2 = df2.replace(to_replace = list(word_freq['words']),value = list(word_freq['represent number']))\n",
    "    df2 = df2[df2[0]!=0]\n",
    "    df2 = df2[0].values.tolist()\n",
    "    Num_whole_set.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>211</td>\n",
       "      <td>48</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>247</td>\n",
       "      <td>248</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>340</td>\n",
       "      <td>24</td>\n",
       "      <td>339</td>\n",
       "      <td>8</td>\n",
       "      <td>432</td>\n",
       "      <td>433</td>\n",
       "      <td>6</td>\n",
       "      <td>52</td>\n",
       "      <td>49</td>\n",
       "      <td>434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>728</td>\n",
       "      <td>779</td>\n",
       "      <td>4</td>\n",
       "      <td>593</td>\n",
       "      <td>211</td>\n",
       "      <td>778</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91</td>\n",
       "      <td>221</td>\n",
       "      <td>62</td>\n",
       "      <td>161</td>\n",
       "      <td>1076</td>\n",
       "      <td>1075</td>\n",
       "      <td>948</td>\n",
       "      <td>1077</td>\n",
       "      <td>527</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2144</td>\n",
       "      <td>2191</td>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "      <td>2192</td>\n",
       "      <td>1</td>\n",
       "      <td>2131</td>\n",
       "      <td>2132</td>\n",
       "      <td>2133</td>\n",
       "      <td>2140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>4</td>\n",
       "      <td>3555</td>\n",
       "      <td>3</td>\n",
       "      <td>3819</td>\n",
       "      <td>62</td>\n",
       "      <td>7078</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>343</td>\n",
       "      <td>9041</td>\n",
       "      <td>1582</td>\n",
       "      <td>154</td>\n",
       "      <td>15464</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>3392</td>\n",
       "      <td>661</td>\n",
       "      <td>343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>13407</td>\n",
       "      <td>1577</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6050</td>\n",
       "      <td>31</td>\n",
       "      <td>2540</td>\n",
       "      <td>26049</td>\n",
       "      <td>4195</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>729</td>\n",
       "      <td>6897</td>\n",
       "      <td>154</td>\n",
       "      <td>7861</td>\n",
       "      <td>754</td>\n",
       "      <td>154</td>\n",
       "      <td>1</td>\n",
       "      <td>39204</td>\n",
       "      <td>39203</td>\n",
       "      <td>13651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>2292</td>\n",
       "      <td>681</td>\n",
       "      <td>93</td>\n",
       "      <td>4</td>\n",
       "      <td>805</td>\n",
       "      <td>385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 759 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3      4     5     6      7      8      9    ...  \\\n",
       "0       211    48    18     8      4   247   248     50      8     16  ...   \n",
       "1       340    24   339     8    432   433     6     52     49    434  ...   \n",
       "2         1   131   728   779      4   593   211    778      6      1  ...   \n",
       "3        91   221    62   161   1076  1075   948   1077    527      7  ...   \n",
       "4      2144  2191     5    49   2192     1  2131   2132   2133   2140  ...   \n",
       "...     ...   ...   ...   ...    ...   ...   ...    ...    ...    ...  ...   \n",
       "1395      4  3555     3  3819     62  7078    13     15     14    683  ...   \n",
       "1396    343  9041  1582   154  15464    65     4   3392    661    343  ...   \n",
       "1397  13407  1577     8     1   6050    31  2540  26049   4195      2  ...   \n",
       "1398    729  6897   154  7861    754   154     1  39204  39203  13651  ...   \n",
       "1399     15    14   223     5   2292   681    93      4    805    385  ...   \n",
       "\n",
       "      749  750  751  752  753  754  755  756  757  758  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "1395  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1396  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1397  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1398  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1399  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1400 rows x 759 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Num_whole_set = pd.DataFrame(Num_whole_set)\n",
    "Num_whole_set = Num_whole_set.fillna(0)\n",
    "Num_whole_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. One can use tokenized text as inputs to a deep neural network. However, a recent breakthrough in NLP suggests that more sophisticated representations of text yield better results. These sophisticated representations are called word embeddings. “Word embedding is a term used for representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.” Most deep learning modules (including Keras) provide a convenient way to convert positive integer representations of words into a word embedding by an “Embedding layer.” The layer accepts arguments that define the mapping of words into embeddings, including the maximum number of expected words also called the vocabulary size (e.g. the largest integer value). The layer also allows you to specify the dimension for each word vector, called the “output dimension.” We would like to use a word embedding layer for this project. Assume that we are interested in the top 5,000 words. This means that in each integer sequence that represents each document, we set to zero those integers that represent words that are not among the top 5,000 words in the document. If you feel more adventurous, use all the words that appear in this corpus. Choose the length of the embedding vector for each word to be 32. Hence, each document is represented as a 32 × L matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>211</td>\n",
       "      <td>48</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>247</td>\n",
       "      <td>248</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>340</td>\n",
       "      <td>24</td>\n",
       "      <td>339</td>\n",
       "      <td>8</td>\n",
       "      <td>432</td>\n",
       "      <td>433</td>\n",
       "      <td>6</td>\n",
       "      <td>52</td>\n",
       "      <td>49</td>\n",
       "      <td>434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>728</td>\n",
       "      <td>779</td>\n",
       "      <td>4</td>\n",
       "      <td>593</td>\n",
       "      <td>211</td>\n",
       "      <td>778</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91</td>\n",
       "      <td>221</td>\n",
       "      <td>62</td>\n",
       "      <td>161</td>\n",
       "      <td>1076</td>\n",
       "      <td>1075</td>\n",
       "      <td>948</td>\n",
       "      <td>1077</td>\n",
       "      <td>527</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2144</td>\n",
       "      <td>2191</td>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "      <td>2192</td>\n",
       "      <td>1</td>\n",
       "      <td>2131</td>\n",
       "      <td>2132</td>\n",
       "      <td>2133</td>\n",
       "      <td>2140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>4</td>\n",
       "      <td>3555</td>\n",
       "      <td>3</td>\n",
       "      <td>3819</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>343</td>\n",
       "      <td>0</td>\n",
       "      <td>1582</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>3392</td>\n",
       "      <td>661</td>\n",
       "      <td>343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>0</td>\n",
       "      <td>1577</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>2540</td>\n",
       "      <td>0</td>\n",
       "      <td>4195</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>729</td>\n",
       "      <td>0</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>754</td>\n",
       "      <td>154</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>2292</td>\n",
       "      <td>681</td>\n",
       "      <td>93</td>\n",
       "      <td>4</td>\n",
       "      <td>805</td>\n",
       "      <td>385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 759 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3     4     5     6     7     8     9    ...  749  \\\n",
       "0      211    48    18     8     4   247   248    50     8    16  ...  0.0   \n",
       "1      340    24   339     8   432   433     6    52    49   434  ...  0.0   \n",
       "2        1   131   728   779     4   593   211   778     6     1  ...  0.0   \n",
       "3       91   221    62   161  1076  1075   948  1077   527     7  ...  0.0   \n",
       "4     2144  2191     5    49  2192     1  2131  2132  2133  2140  ...  0.0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...  ...   \n",
       "1395     4  3555     3  3819    62     0    13    15    14   683  ...  0.0   \n",
       "1396   343     0  1582   154     0    65     4  3392   661   343  ...  0.0   \n",
       "1397     0  1577     8     1     0    31  2540     0  4195     2  ...  0.0   \n",
       "1398   729     0   154     0   754   154     1     0     0     0  ...  0.0   \n",
       "1399    15    14   223     5  2292   681    93     4   805   385  ...  0.0   \n",
       "\n",
       "      750  751  752  753  754  755  756  757  758  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "1395  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1396  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1397  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1398  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1399  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1400 rows x 759 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Num_whole_set[Num_whole_set > 5000] = 0\n",
    "Num_whole_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_whole_data5000 = np.array(Num_whole_set).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 14:26:41.686149: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(5001, 32, input_length=759))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1400, 759, 32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array_whole = model.predict(np.array(Num_whole_data5000))\n",
    "output_array_whole.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00259569,  0.02768074, -0.00018419, ...,  0.03184338,\n",
       "          0.0218347 ,  0.0375353 ],\n",
       "        [-0.04962156, -0.02289253,  0.02575288, ...,  0.00885042,\n",
       "          0.02593607, -0.00737177],\n",
       "        [-0.03259841,  0.04989113, -0.02369815, ...,  0.03709881,\n",
       "          0.03540025, -0.03875468],\n",
       "        ...,\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522],\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522],\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522]],\n",
       "\n",
       "       [[ 0.0401021 ,  0.02497467,  0.02253545, ...,  0.02666571,\n",
       "          0.02585876,  0.04705978],\n",
       "        [-0.02749499, -0.01330737, -0.00429729, ...,  0.03179636,\n",
       "          0.0287824 , -0.01339125],\n",
       "        [-0.04585086, -0.02159567,  0.04832203, ...,  0.01252755,\n",
       "          0.01398728,  0.04988604],\n",
       "        ...,\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522],\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522],\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522]],\n",
       "\n",
       "       [[-0.00469957, -0.00744319,  0.0477486 , ..., -0.03846114,\n",
       "          0.02306129, -0.0018533 ],\n",
       "        [ 0.02894158, -0.008222  ,  0.00473294, ..., -0.0192914 ,\n",
       "          0.02011564, -0.03895751],\n",
       "        [ 0.03339704, -0.01752833,  0.0288063 , ...,  0.01128726,\n",
       "          0.0351016 ,  0.0479739 ],\n",
       "        ...,\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522],\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522],\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522],\n",
       "        [ 0.04180672, -0.01935052, -0.03894421, ..., -0.02991281,\n",
       "          0.01405862, -0.04951583],\n",
       "        [-0.03846562, -0.01883745, -0.00660545, ...,  0.03655878,\n",
       "          0.01468749,  0.03204304],\n",
       "        ...,\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522],\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522],\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522]],\n",
       "\n",
       "       [[ 0.03040547, -0.00262501,  0.0167521 , ..., -0.03313683,\n",
       "          0.0040956 , -0.00114471],\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522],\n",
       "        [ 0.03431804, -0.02491728,  0.01639876, ..., -0.04454571,\n",
       "          0.02405736,  0.04481337],\n",
       "        ...,\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522],\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522],\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522]],\n",
       "\n",
       "       [[-0.01638781,  0.03272951, -0.04360377, ...,  0.02788964,\n",
       "         -0.02536705,  0.04484257],\n",
       "        [-0.03796116, -0.00929989, -0.02086909, ..., -0.01199604,\n",
       "         -0.02586818,  0.04502142],\n",
       "        [ 0.03394481, -0.01237762,  0.0495842 , ...,  0.04249075,\n",
       "          0.00090245, -0.03956883],\n",
       "        ...,\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522],\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522],\n",
       "        [-0.02885747,  0.04521516,  0.04332782, ...,  0.03752861,\n",
       "          0.03748882, -0.03122522]]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = whole_data.iloc[:1400]\n",
    "test_set = whole_data.iloc[1400:]\n",
    "Truncate_train_set = train_set[train_set['length'] < 759.3]\n",
    "Truncate_test_set = test_set[test_set['length'] < 759.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(Truncate_train_set['review'].str.split(' ')).reset_index().drop('index', axis=1)\n",
    "df_test = pd.DataFrame(Truncate_test_set['review'].str.split(' ')).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_train_set = Num_whole_set.iloc[:991]\n",
    "Num_test_set = Num_whole_set.iloc[991:]\n",
    "\n",
    "X_train = np.array(Num_train_set).astype(int)\n",
    "X_test = np.array(Num_test_set).astype(int)\n",
    "\n",
    "y_train = Truncate_train_set['y']\n",
    "y_test = Truncate_test_set['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(5001, 32, input_length=759))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.00216504, -0.00718103,  0.03416915, ..., -0.02517171,\n",
       "          0.01732892,  0.00484704],\n",
       "        [ 0.02112349, -0.00522206,  0.00323321, ..., -0.01871302,\n",
       "         -0.04686619, -0.0001593 ],\n",
       "        [-0.01601249, -0.03416531,  0.00979795, ..., -0.02229114,\n",
       "         -0.00686778, -0.03327729],\n",
       "        ...,\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533]],\n",
       "\n",
       "       [[-0.00839268, -0.04125232,  0.0356405 , ...,  0.0256269 ,\n",
       "         -0.00870531,  0.01888131],\n",
       "        [ 0.0437346 ,  0.03654002, -0.0467484 , ...,  0.03398604,\n",
       "         -0.00250373,  0.03221243],\n",
       "        [-0.00840569, -0.01055732, -0.03299376, ..., -0.01255123,\n",
       "         -0.04240806, -0.01594251],\n",
       "        ...,\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533]],\n",
       "\n",
       "       [[-0.01497315,  0.02942686,  0.00658073, ..., -0.02583437,\n",
       "         -0.04227408, -0.03439246],\n",
       "        [ 0.02767079, -0.04341922,  0.00208615, ..., -0.0357833 ,\n",
       "          0.00268279, -0.03441241],\n",
       "        [-0.02156936,  0.04908592, -0.0290166 , ..., -0.01082464,\n",
       "         -0.02102183,  0.04195484],\n",
       "        ...,\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [ 0.03618317, -0.03879768, -0.02134399, ...,  0.0148574 ,\n",
       "         -0.03073214, -0.01444463],\n",
       "        [-0.01497315,  0.02942686,  0.00658073, ..., -0.02583437,\n",
       "         -0.04227408, -0.03439246],\n",
       "        ...,\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533]],\n",
       "\n",
       "       [[-0.03697472, -0.04993732, -0.01482186, ...,  0.00588945,\n",
       "         -0.03823674, -0.00885644],\n",
       "        [ 0.01245636,  0.03929729,  0.01274988, ..., -0.00857598,\n",
       "         -0.01283975, -0.037445  ],\n",
       "        [-0.01497315,  0.02942686,  0.00658073, ..., -0.02583437,\n",
       "         -0.04227408, -0.03439246],\n",
       "        ...,\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533]],\n",
       "\n",
       "       [[-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.00966859, -0.01138901,  0.00491626, ..., -0.01331469,\n",
       "          0.02020345,  0.04032059],\n",
       "        [ 0.00578542, -0.04520694,  0.01522033, ..., -0.03433831,\n",
       "         -0.03397814,  0.01170118],\n",
       "        ...,\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533]]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = model2.predict(np.array(X_train))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 992us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.01144792, -0.01702   ,  0.03462261, ...,  0.04191   ,\n",
       "          0.04091659,  0.01465794],\n",
       "        [ 0.03570965,  0.00106177, -0.01420692, ...,  0.00862787,\n",
       "          0.02130147, -0.04562652],\n",
       "        [-0.01720103, -0.0129248 ,  0.01854545, ...,  0.03699848,\n",
       "          0.02241892,  0.04778579],\n",
       "        ...,\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533]],\n",
       "\n",
       "       [[-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [ 0.03744313,  0.04434112,  0.0370584 , ..., -0.04195363,\n",
       "         -0.02637284, -0.00668135],\n",
       "        [-0.03983171,  0.03353425, -0.04032286, ..., -0.04526197,\n",
       "          0.02929338,  0.03142948],\n",
       "        ...,\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533]],\n",
       "\n",
       "       [[ 0.00728097,  0.04410163,  0.03857851, ...,  0.02648947,\n",
       "          0.04841571, -0.03786129],\n",
       "        [ 0.01836859,  0.03700374,  0.02748827, ..., -0.0303016 ,\n",
       "         -0.04026636,  0.0115232 ],\n",
       "        [-0.02466149, -0.00241662, -0.00659809, ...,  0.01203158,\n",
       "          0.01329738, -0.0034445 ],\n",
       "        ...,\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.0223708 ,  0.00771396, -0.00562371, ...,  0.01880738,\n",
       "          0.02690889,  0.03072609],\n",
       "        [ 0.0259379 ,  0.01651369,  0.00762993, ..., -0.00277954,\n",
       "          0.01541538,  0.00075374],\n",
       "        ...,\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533]],\n",
       "\n",
       "       [[-0.0052531 ,  0.03485331,  0.02898705, ..., -0.02594577,\n",
       "         -0.03319984, -0.01681303],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [ 0.04579988,  0.03861209,  0.01963266, ...,  0.01990508,\n",
       "          0.03183288,  0.04997312],\n",
       "        ...,\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533]],\n",
       "\n",
       "       [[-0.00187142,  0.02582479,  0.01696115, ..., -0.02917881,\n",
       "         -0.04395619, -0.04982479],\n",
       "        [ 0.04345858,  0.01760757,  0.01080563, ..., -0.03009131,\n",
       "         -0.01567367, -0.03041791],\n",
       "        [-0.0072696 , -0.00538931, -0.04525476, ..., -0.00046914,\n",
       "          0.02367621, -0.04327284],\n",
       "        ...,\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533],\n",
       "        [-0.03675704, -0.03498635, -0.01624339, ..., -0.00584795,\n",
       "          0.0015322 ,  0.00912533]]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = model2.predict(np.array(X_test))\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. Flatten the matrix of each document to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flatten = []\n",
    "for i in range(len(X_train)):\n",
    "    vec = X_train[i].flatten()\n",
    "    X_train_flatten.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_flatten = []\n",
    "for i in range(len(X_test)):\n",
    "    vec = X_test[i].flatten()\n",
    "    X_test_flatten.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_flatten = []\n",
    "for i in range(len(output_array_whole)):\n",
    "    vec = output_array_whole[i].flatten()\n",
    "    whole_flatten.append(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. Train a MLP with three (dense) hidden layers each of which has 50 ReLUs and one output layer with a single sigmoid neuron. Use a dropout rate of 20% for the first layer and 50% for the other layers. Use ADAM optimizer and binary cross entropy loss (which is equivalent to having a softmax in the output). To avoid overfitting, just set the number of epochs as 2. Use a batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\t# create model\n",
    "\tmodel3 = Sequential()\n",
    "\tmodel3.add(Dropout(0.2, input_shape=(24288,)))\n",
    "\tmodel3.add(Dense(50, activation='relu'))\n",
    "\tmodel3.add(Dropout(0.5))\n",
    "\tmodel3.add(Dense(50, activation='relu'))\n",
    "\tmodel3.add(Dropout(0.5))\n",
    "\tmodel3.add(Dense(50, activation='relu'))\n",
    "\tmodel3.add(Dropout(0.5))\n",
    "\tmodel3.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=create_model, epochs=2, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_set = pd.DataFrame(X_train_flatten)\n",
    "y_train_set = pd.DataFrame(y_train).reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb10ec8fe80>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(X_train_set,y_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5469223007063572"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train accuracy\n",
    "y_pred = estimator.predict(X_train_set)\n",
    "accuracy_score(y_train_set, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_set = pd.DataFrame(X_test_flatten)\n",
    "y_test_set = pd.DataFrame(y_test).reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5403422982885085"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy\n",
    "y_pred = estimator.predict(X_test_set)\n",
    "accuracy_score(y_test_set, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) One-Dimensional Convolutional Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Although CNNs are mainly used for image data, they can also be applied to text data, as text also has adjacency information. Keras supports one-dimensional convolutions and pooling by the Conv1D and MaxPooling1D classes respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. After the embedding layer, insert a Conv1D layer. This convolutional layer has 32 feature maps , and each of the 32 kernels has size 3, i.e. reads embedded word representations 3 vector elements of the word embedding at a time. The convolutional layer is followed by a 1D max pooling layer with a length and stride of 2 that halves the size of the feature maps from the convolutional layer. The rest of the network is the same as the neural network above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model():\n",
    "\t# create model\n",
    "\tinputs1 = Input(shape=(759,))\n",
    "\tembedding1 = Embedding(5001, 32)(inputs1)\n",
    "\tconv1 = Conv1D(filters=32, kernel_size=3, activation='relu')(embedding1)\n",
    "\tdrop1 = Dropout(0.2)(conv1)\n",
    "\tpool1 = MaxPooling1D(pool_size=2, strides=2)(drop1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "\tdense1 = Dense(50, activation='relu')(flat1)\n",
    "\tdrop2 = Dropout(0.5)(dense1)\n",
    "\tdense2 = Dense(50, activation='relu')(drop2)\n",
    "\tdrop3 = Dropout(0.5)(dense2)\n",
    "\tdense3 = Dense(50, activation='relu')(drop3)\n",
    "\tdrop4 = Dropout(0.5)(dense3)\n",
    "\toutputs = Dense(1, activation='sigmoid')(drop4)\n",
    "\tmodel_conv = Model(inputs=inputs1, outputs=outputs)\n",
    "\tmodel_conv.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = pd.DataFrame(np.array(Num_train_set).astype(int))\n",
    "testX = pd.DataFrame(np.array(Num_test_set).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator2 = KerasClassifier(build_fn=define_model, epochs=2, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb0a62f6d30>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator2.fit(trainX,y_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5348133198789102"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train accuracy\n",
    "y_pred = estimator2.predict(trainX)\n",
    "accuracy_score(y_train_set,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5403422982885085"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy\n",
    "y_pred = estimator2.predict(testX)\n",
    "accuracy_score(y_test_set,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Long Short-Term Memory Recurrent Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The structure of the LSTM we are going to use is shown in the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. Each word is represented to LSTM as a vector of 32 elements and the LSTM is followed by a dense layer of 256 ReLUs. Use a dropout rate of 0.2 for both LSTM and the dense layer. Train the model using 10-50 epochs and batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "def build_model():\n",
    "\t# create model\n",
    "\tmodel_LSTM = Sequential()\n",
    "\tmodel_LSTM.add(Embedding(5001, 32, input_length=759))\n",
    "\tmodel_LSTM.add(LSTM(32))\n",
    "\tmodel_LSTM.add(Dropout(0.2))\n",
    "\tmodel_LSTM.add(Dense(256, activation='relu'))\n",
    "\tmodel_LSTM.add(Dropout(0.2))\n",
    "\tmodel_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel_LSTM.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator3 = KerasClassifier(build_fn=build_model, epochs=10, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb0a1f9a7f0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator3.fit(trainX,y_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 1s 36ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5378405650857719"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train accuracy\n",
    "y_pred = estimator3.predict(trainX)\n",
    "accuracy_score(y_train_set,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 37ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5403422982885085"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy\n",
    "y_pred = estimator3.predict(testX)\n",
    "accuracy_score(y_test_set,y_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df14f7d3090333b7145699d502428ad48cfb4c76053bb1c2b241871c04360d45"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
